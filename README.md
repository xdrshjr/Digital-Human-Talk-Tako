# DICE-Talk
Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation.


<a href='https://toto222.github.io/DICE-Talk/'><img src='https://img.shields.io/badge/Project-Page-Green'></a>
<a href='https://arxiv.org/abs/2504.18087'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>
<a href="https://raw.githubusercontent.com/toto222/DICE-Talk/refs/heads/main/LICENSE" style="margin: 0 2px;">
  <img src='https://img.shields.io/badge/License-CC BY--NC--SA--4.0-lightgreen?style=flat&logo=Lisence' alt='License'>
</a>




## ðŸ”¥ðŸ”¥ðŸ”¥ NEWS

**`2025/04/29`**: We released the initial version of the inference code and models. Stay tuned for continuous updates!



## ðŸŽ¥ Demo
| Input                | Neutral                | Happy                | Angry                | Surprised
|----------------------|-----------------------|----------------------|-----------------------|-----------------------|
|<img src="examples/img/female.png" width="640">|<video src="https://github.com/user-attachments/assets/e17ccff2-12f3-4d0e-8475-ce0e2dd6bd2a" width="320"> </video>|<video src="https://github.com/user-attachments/assets/cf799a36-c489-453f-85a7-dce7f366e0f0" width="320"> </video>|<video src="https://github.com/user-attachments/assets/c30c39f8-ab5d-4382-837d-b26137edbdd8" width="320"> </video>|<video src="https://github.com/user-attachments/assets/5f24b0dc-2f43-46c9-90bd-cc9e635be014" width="320"> </video>|
|<img src="examples/img/pyy.jpg" width="640">|<video src="https://github.com/user-attachments/assets/629753bc-aad0-45f3-bc0b-b6b8eb599f17" width="320"> </video>|<video src="https://github.com/user-attachments/assets/8619ef3d-4669-45ee-9cce-3f21df6d4bb3" width="320"> </video>|<video src="https://github.com/user-attachments/assets/79bae96b-175b-4dd4-8d4e-6f325959f67f" width="320"> </video>|<video src="https://github.com/user-attachments/assets/f3f7287d-e0b9-466d-abf2-019ef44f5ace" width="320"> </video>|




For more visual demos, please visit our [**Page**](https://toto222.github.io/DICE-Talk/).



## ðŸ“œ Requirements
* It is recommended to use a GPU with `20GB` or more VRAM and have an independent `Python 3.10`.
* Tested operating system: `Linux`

## ðŸ”‘ Inference

### Installtion
- `ffmpeg` requires to be installed.
- `PyTorch`: make sure to select the appropriate CUDA version based on your hardware, for example,
```shell
pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118
```
- `Dependencies`:
```shell
pip install -r requirements.txt
```
- All models are stored in `checkpoints` by default, and the file structure is as follows:
```shell
DICE-Talk
  â”œâ”€â”€checkpoints
  â”‚  â”œâ”€â”€DICE-Talk
  â”‚  â”‚  â”œâ”€â”€audio_linear.pth
  â”‚  â”‚  â”œâ”€â”€emo_model.pth
  â”‚  â”‚  â”œâ”€â”€pose_guider.pth  
  â”‚  â”‚  â”œâ”€â”€unet.pth
  â”‚  â”œâ”€â”€stable-video-diffusion-img2vid-xt
  â”‚  â”‚  â”œâ”€â”€...
  â”‚  â”œâ”€â”€whisper-tiny
  â”‚  â”‚  â”œâ”€â”€...
  â”‚  â”œâ”€â”€RIFE
  â”‚  â”‚  â”œâ”€â”€flownet.pkl
  â”‚  â”œâ”€â”€yoloface_v5m.pt
  â”œâ”€â”€...
```
Download by `huggingface-cli` follow
```shell
python3 -m pip install "huggingface_hub[cli]"

huggingface-cli download EEEELY/DICE-Talk --local-dir  checkpoints
huggingface-cli download stabilityai/stable-video-diffusion-img2vid-xt --local-dir  checkpoints/stable-video-diffusion-img2vid-xt
huggingface-cli download openai/whisper-tiny --local-dir checkpoints/whisper-tiny
```

or manully download [pretrain model](https://drive.google.com/drive/folders/1l1Ojt-4yMfYQCCnNs_NgkzQC2-OoAksN?usp=drive_link), [svd-xt](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt) and [whisper-tiny](https://huggingface.co/openai/whisper-tiny) to `checkpoints/`.


### Run demo
```shell
python3 demo.py --image_path '/path/to/input_image' --audio_path '/path/to/input_audio'\ 
  --emotion_path '/path/to/input_emotion' --output_path '/path/to/output_video'
```

### Run GUI
```shell
python3 gradio_app.py
```

<img width="720" alt="gradio_demo" src="https://github.com/user-attachments/assets/7cdb2e6b-53c4-43e4-b6df-2b25db10ea8d" />



On the left you need to:
* Upload an image or take a photo
* Upload or record an audio clip
* Select the type of emotion to generate
* Set the strength for identity preservation and emotion generation
* Choose whether to crop the input image

On the right are the generated videos.

 
## ðŸ”— Citation

If you find our work helpful for your research, please consider citing our work.   

```bibtex
@misc{tan2025disentangleidentitycooperateemotion,
      title={Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation}, 
      author={Weipeng Tan and Chuming Lin and Chengming Xu and FeiFan Xu and Xiaobin Hu and Xiaozhong Ji and Junwei Zhu and Chengjie Wang and Yanwei Fu},
      year={2025},
      eprint={2504.18087},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.18087}, 
}
```
